<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/favicon/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/favicon/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><link rel="apple-touch-icon" href="/favicon/apple-touch-icon.png?v=2.6.2" sizes="180x180"><meta name="description" content="PyTorch 中的张量（Tensor）实现。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch 源码阅读：c10">
<meta property="og:url" content="https://aurumting.cn/2021/12/04/PyTorch_c10/index.html">
<meta property="og:site_name" content="Aurumting&#39;s Blog">
<meta property="og:description" content="PyTorch 中的张量（Tensor）实现。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-12-04T13:54:42.039Z">
<meta property="article:modified_time" content="2021-12-15T11:39:02.406Z">
<meta property="article:author" content="Aurumting">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary"><title>PyTorch 源码阅读：c10 | Aurumting's Blog</title><link ref="canonical" href="https://aurumting.cn/2021/12/04/PyTorch_c10/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: undefined,
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: true,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Aurumting's Blog" type="application/atom+xml">
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">PyTorch 源码阅读：c10</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-12-04</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-12-15</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon"><i class="fas fa-eye"></i></span><span class="post-meta-item__info">阅读次数</span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"><p>PyTorch 中的张量（Tensor）实现。</p>
<span id="more"></span>

        <h1 id="uniquevoidptr"   >
          <a href="#uniquevoidptr" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#uniquevoidptr"></a> UniqueVoidPtr</h1>
      
<p><code>detail::UniqueVoidPtr</code> 是一个像 <code>unique_ptr</code> 一样的独有智能指针，但有三个主要区别：</p>
<ol>
<li>它是专门针对 <code>void</code> 的</li>
<li>它专门用于函数指针删除器 <code>void(void* ctx)</code> ；也就是说，删除器不接受对数据的引用，只接受一个上下文指针（作为 <code>void*</code> 被删除）。事实上，在内部，这个指针被实现为有一个对上下文的独有引用，和一个对数据的非独有引用；这就是为什么你要 <code>release_context()</code> ，而不是 <code>release()</code>（传统的 <code>release()</code> 的 API 不会给你足够的信息来正确处置这个对象）</li>
<li>当 <code>unique</code> 指针被析构并且上下文非空时，保证删除器被调用；这与 <code>std::unique_ptr</code> 不同，在后者中，如果数据指针为空，删除器不被调用</li>
</ol>
<p>一些方法的类型与 <code>std::unique_ptr</code> 略有不同，以反映这一点。</p>
<p><code>UniqueVoidPtr</code> 类：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UniqueVoidPtr</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">void</span>* data_;</span><br><span class="line">    std::unique_ptr&lt;<span class="keyword">void</span>, DeleterFnPtr&gt; ctx_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">UniqueVoidPtr</span>() : <span class="built_in">data_</span>(<span class="literal">nullptr</span>), <span class="built_in">ctx_</span>(<span class="literal">nullptr</span>, &amp;deleteNothing) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">UniqueVoidPtr</span><span class="params">(<span class="keyword">void</span>* data)</span></span></span><br><span class="line"><span class="function">        : data_(data), ctx_(nullptr, &amp;deleteNothing) &#123;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">UniqueVoidPtr</span>(<span class="keyword">void</span>* data, <span class="keyword">void</span>* ctx, DeleterFnPtr ctx_deleter)</span><br><span class="line">        : <span class="built_in">data_</span>(data), <span class="built_in">ctx_</span>(ctx, ctx_deleter ? ctx_deleter : &amp;deleteNothing) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span>* <span class="keyword">operator</span>-&gt;() <span class="keyword">const</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> data_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ctx_ = <span class="literal">nullptr</span>;</span><br><span class="line">        data_ = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">get</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">get_context</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ctx_.<span class="built_in">get</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">release_context</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ctx_.<span class="built_in">release</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="keyword">void</span>, DeleterFnPtr&gt;&amp;&amp; <span class="title">move_context</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">move</span>(ctx_);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">C10_NODISCARD <span class="keyword">bool</span> <span class="title">compare_exchange_deleter</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        DeleterFnPtr expected_deleter,</span></span></span><br><span class="line"><span class="params"><span class="function">        DeleterFnPtr new_deleter)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">get_deleter</span>() != expected_deleter)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        ctx_ = std::unique_ptr&lt;<span class="keyword">void</span>, DeleterFnPtr&gt;(ctx_.<span class="built_in">release</span>(), new_deleter);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function">T* <span class="title">cast_context</span><span class="params">(DeleterFnPtr expected_deleter)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">get_deleter</span>() != expected_deleter)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;T*&gt;(<span class="built_in">get_context</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">operator</span> <span class="title">bool</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data_ || ctx_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">DeleterFnPtr <span class="title">get_deleter</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ctx_.<span class="built_in">get_deleter</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
<p>其中：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> DeleterFnPtr = <span class="built_in"><span class="keyword">void</span></span> (*)(<span class="keyword">void</span>*)</span><br></pre></td></tr></table></div></figure>
<p>也就是说：<code>DeleterFnPtr</code> 是一个函数指针，接受一个无类型指针，无返回类型。</p>
<p>以及 <code>deleteNoting</code> 声明：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TORCH_API <span class="keyword">void</span> <span class="title">deleteNothing</span><span class="params">(<span class="keyword">void</span>*)</span></span>;</span><br></pre></td></tr></table></div></figure>
<p>及其实现：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteNothing</span><span class="params">(<span class="keyword">void</span>*)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></div></figure>
<p>顾名思义，<code>deleteNothing</code> 什么都不做。</p>
<p><code>UniqueVoidPtr</code> 中有两个私有成员变量：无类型指针 <code>data_</code> 以及带有删除器 <code>DeleterFnPtr</code> 的指向 <code>void</code> 的独有指针 <code>ctx_</code> 。只要区分了数据和上下文，其他的方法都可以从名字中理解其含义。</p>
<p><code>UniqueVoidPtr</code> 解决了张量数据分配器的一个常见问题，即你感兴趣的数据指针（例如 <code>float*</code>）与你需要实际释放数据的上下文指针（例如 <code>DLManagedTensor</code>）不一致。在传统的删除器设计中，你必须在删除器本身存储额外的上下文，这样你才能真正删除正确的东西。用标准的 C++ 实现这一点有点容易出错：如果你使用 <code>std::unique_ptr</code> 来管理张量，如果数据指针是 <code>nullptr</code> ，删除器将不会被调用，如果上下文指针是非空的，这可能会导致泄漏（而删除器负责释放数据指针和上下文指针）。</p>
<p>因此，在我们对 <code>unique_ptr</code> 的重新实现中，只是将上下文直接存储在 <code>unique</code> 指针中，并将删除器附加到上下文指针本身。在简单的情况下，上下文指针只是指针本身。</p>

        <h1 id="allocator"   >
          <a href="#allocator" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#allocator"></a> Allocator</h1>
      
<p>一个 <code>DataPtr</code> 是一个的指向一些内存的 <code>unique</code> 指针（有一个附加的删除器和一些删除器的上下文），它也记录了其数据的设备是什么。</p>
<p><code>nullptr</code> <code>DataPtr</code> 仍然可以有一个非平凡的设备；这允许我们将零大小的分配与非零分配统一对待。</p>
<p><code>DatePtr</code> 类（删减版）：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C10_API</span> <span class="title">DataPtr</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    c10::detail::UniqueVoidPtr ptr_;</span><br><span class="line">    Device device_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">DataPtr</span>() : <span class="built_in">ptr_</span>(), <span class="built_in">device_</span>(DeviceType::CPU) &#123;&#125;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">void</span>* <span class="keyword">operator</span>-&gt;() <span class="keyword">const</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ptr_.<span class="built_in">get</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* some getter-setter method : call ptr_ */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">operator</span> <span class="title">bool</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;<span class="keyword">bool</span>&gt;(ptr_);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function">T* <span class="title">cast_context</span><span class="params">(DeleterFnPtr expected_deleter)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ptr_.cast_context&lt;T&gt;(expected_deleter);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function">C10_NODISCARD <span class="keyword">bool</span> <span class="title">compare_exchange_deleter</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        DeleterFnPtr expected_deleter,</span></span></span><br><span class="line"><span class="params"><span class="function">        DeleterFnPtr new_deleter)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ptr_.<span class="built_in">compare_exchange_deleter</span>(expected_deleter, new_deleter);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
<p><code>DataPtr</code> 有两个成员变量：之前介绍的 <code>UniqueVoidPtr</code> 类型的 <code>ptr_</code> 以及设备类型的 <code>device_</code> 。大部分方法也就是去调用 <code>ptr_</code> 相应的方法。</p>
<p><code>Allocator</code> 类：</p>
<p>在某些情况下，上下文与数据指针完全相同。在这种情况下，我们可以支持 <code>raw</code> 分配和释放接口。这就是 <code>raw_deleter</code> 的含义。默认情况下，它返回一个 <code>nullptr</code> ，这意味着 <code>raw</code> 接口没有被实现。请确保尽可能地实现它，否则 <code>raw</code> 接口将被错误地报告为不支持，而实际上它是可能的。</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">Allocator</span> &#123;</span></span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">Allocator</span>() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DataPtr <span class="title">allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DeleterFnPtr <span class="title">raw_deleter</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">raw_allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> dptr = <span class="built_in">allocate</span>(n);</span><br><span class="line">    <span class="built_in">AT_ASSERT</span>(dptr.<span class="built_in">get</span>() == dptr.<span class="built_in">get_context</span>());</span><br><span class="line">    <span class="keyword">return</span> dptr.<span class="built_in">release_context</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">raw_deallocate</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> d = <span class="built_in">raw_deleter</span>();</span><br><span class="line">    <span class="built_in">AT_ASSERT</span>(d);</span><br><span class="line">    <span class="built_in">d</span>(ptr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>

        <h1 id="storage"   >
          <a href="#storage" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#storage"></a> Storage</h1>
      
<p>存储器代表张量的底层支持数据缓冲区。这个概念是从最初的 <code>Torch7</code> 代码库中继承下来的；我们有点想摆脱这个概念，但这是一项艰苦的工作，而且没有人去做。</p>
<p>注意：存储应该是唯一拥有数据指针的；例如，两个非空的数据指针只有当它们来自同一个存储时才有别名。从技术上讲，你可以违反这个不变性（例如，你可以用 <code>at::from_blob</code> 创建一个不拥有的 <code>StorageImpl</code>），但是很多事情都不能正常工作，包括：</p>
<ul>
<li>在这样的存储上使用普通的删除器是错误的，因为普通的删除器假设是唯一的所有权，但如果你有两个存储在同一个数据，这意味着存在某种共享的所有权。所以你的删除器实际上必须在内部做某种引用计数的事情</li>
<li>Python 方面的深复制依赖于存储的平等，而不是数据指针的平等；所以如果有两个独立的存储指向同一个数据，在这种情况下，数据实际上会被重复（一个数据指针在前，两个数据指针在后）</li>
<li>版本计数不会正常工作，因为我们在存储层面上进行所有的版本计数跟踪（除非你明确地用 <code>detach</code> 断开版本计数的连接）；由于数据指针相同的突变是完全没有被跟踪的</li>
</ul>
<p><code>Storage</code> 类（删减版）：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">Storage</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">use_byte_size_t</span> &#123;</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Storage</span>() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Storage</span>(c10::intrusive_ptr&lt;StorageImpl&gt; ptr)</span><br><span class="line">        : <span class="built_in">storage_impl_</span>(std::<span class="built_in">move</span>(ptr)) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Storage</span>(</span><br><span class="line">        <span class="keyword">use_byte_size_t</span> use_byte_size,</span><br><span class="line">        <span class="keyword">size_t</span> size_bytes,</span><br><span class="line">        Allocator* allocator = <span class="literal">nullptr</span>,</span><br><span class="line">        <span class="keyword">bool</span> resizable = <span class="literal">false</span>)</span><br><span class="line">        : <span class="built_in">storage_impl_</span>(c10::make_intrusive&lt;StorageImpl&gt;(</span><br><span class="line">                StorageImpl::<span class="built_in">use_byte_size_t</span>(),</span><br><span class="line">                size_bytes,</span><br><span class="line">                allocator,</span><br><span class="line">                resizable)) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function">T* <span class="title">data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> storage_impl_-&gt;data&lt;T&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">is_alias_of</span><span class="params">(<span class="keyword">const</span> Storage&amp; other)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> storage_impl_ == other.storage_impl_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">UniqueStorageShareExternalPointer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">void</span>* src,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">size_t</span> capacity,</span></span></span><br><span class="line"><span class="params"><span class="function">        DeleterFnPtr d = <span class="literal">nullptr</span>)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!storage_impl_.<span class="built_in">unique</span>()) &#123;</span><br><span class="line">            <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">                <span class="literal">false</span>,</span><br><span class="line">                <span class="string">&quot;UniqueStorageShareExternalPointer can only be called when use_count == 1&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        storage_impl_-&gt;<span class="built_in">UniqueStorageShareExternalPointer</span>(src, capacity, d);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    c10::intrusive_ptr&lt;StorageImpl&gt; storage_impl_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
<p>里面大部分方法都是调用了下面的：<br />
<code>StorageImpl</code> 类（删减版）：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">StorageImpl</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">use_byte_size_t</span> &#123;</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">StorageImpl</span>(</span><br><span class="line">        <span class="keyword">use_byte_size_t</span> use_byte_size,</span><br><span class="line">        <span class="keyword">size_t</span> size_bytes,</span><br><span class="line">        at::DataPtr data_ptr,</span><br><span class="line">        at::Allocator* allocator,</span><br><span class="line">        <span class="keyword">bool</span> resizable)</span><br><span class="line">        : <span class="built_in">data_ptr_</span>(std::<span class="built_in">move</span>(data_ptr)),</span><br><span class="line">            <span class="built_in">size_bytes_</span>(size_bytes),</span><br><span class="line">            <span class="built_in">resizable_</span>(resizable),</span><br><span class="line">            <span class="built_in">received_cuda_</span>(<span class="literal">false</span>),</span><br><span class="line">            <span class="built_in">allocator_</span>(allocator) &#123;</span><br><span class="line">        <span class="keyword">if</span> (resizable) &#123;</span><br><span class="line">            <span class="built_in">TORCH_INTERNAL_ASSERT</span>(</span><br><span class="line">                allocator_, <span class="string">&quot;For resizable storage, allocator must be provided&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* other constructor, destructor and operator= */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        data_ptr_.<span class="built_in">clear</span>();</span><br><span class="line">        size_bytes_ = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> T* <span class="title">data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> unsafe_data&lt;T&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> T* <span class="title">unsafe_data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;T*&gt;(<span class="keyword">this</span>-&gt;data_ptr_.<span class="built_in">get</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">release_resources</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        data_ptr_.<span class="built_in">clear</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* some getter-setter method */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Returns the previous data_ptr</span></span><br><span class="line">    <span class="function">at::DataPtr <span class="title">set_data_ptr</span><span class="params">(at::DataPtr&amp;&amp; data_ptr)</span> </span>&#123;</span><br><span class="line">        std::<span class="built_in">swap</span>(data_ptr_, data_ptr);</span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">move</span>(data_ptr);</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Can only be called when use_count is 1</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">UniqueStorageShareExternalPointer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        at::DataPtr&amp;&amp; data_ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">size_t</span> size_bytes)</span> </span>&#123;</span><br><span class="line">        data_ptr_ = std::<span class="built_in">move</span>(data_ptr);</span><br><span class="line">        size_bytes_ = size_bytes;</span><br><span class="line">        allocator_ = <span class="literal">nullptr</span>;</span><br><span class="line">        resizable_ = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    DataPtr data_ptr_;</span><br><span class="line">    <span class="keyword">size_t</span> size_bytes_;</span><br><span class="line">    <span class="keyword">bool</span> resizable_;</span><br><span class="line">    <span class="comment">// Identifies that Storage was received from another process and doesn&#x27;t have</span></span><br><span class="line">    <span class="comment">// local to process cuda memory allocation</span></span><br><span class="line">    <span class="keyword">bool</span> received_cuda_;</span><br><span class="line">    Allocator* allocator_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
<p><code>StorageImpl</code> 有 5 个成员变量，主要有 <code>DataPtr</code> 类型的 <code>data_ptr_</code> 和指向 <code>Allocator</code> 的指针 <code>allocator_</code> 。</p>

        <h1 id="tensorimpl"   >
          <a href="#tensorimpl" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#tensorimpl"></a> TensorImpl</h1>
      

        <h2 id="placementdeletecontext"   >
          <a href="#placementdeletecontext" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#placementdeletecontext"></a> PlacementDeleteContext</h2>
      
<p>首先介绍一个将在解构期间调用额外的放置删除器的上下文 <code>PlacementDeleteContext</code> 。</p>
<p>接受一个已经构建好的 <code>DataPtr</code> 并在销毁时将其存储为成员，我们将在  <code>DataPtr</code> 被销毁之前调用底层数据指针的额外删除器。<code>data_ptr_</code> 拥有该内存。</p>
<p><code>PlacementDeleteContext</code> 类：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">PlacementDeleteContext</span> &#123;</span></span><br><span class="line">    DataPtr data_ptr_;</span><br><span class="line">    PlacementDtor placement_dtor_;</span><br><span class="line">    <span class="keyword">size_t</span> size_;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PlacementDeleteContext</span>(</span><br><span class="line">        DataPtr&amp;&amp; data_ptr,</span><br><span class="line">        PlacementDtor placement_dtor,</span><br><span class="line">        <span class="keyword">size_t</span> size)</span><br><span class="line">        : <span class="built_in">data_ptr_</span>(std::<span class="built_in">move</span>(data_ptr)),</span><br><span class="line">            <span class="built_in">placement_dtor_</span>(placement_dtor),</span><br><span class="line">            <span class="built_in">size_</span>(size) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> DataPtr <span class="title">makeDataPtr</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        DataPtr&amp;&amp; data_ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">        PlacementDtor placement_dtor,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">size_t</span> size,</span></span></span><br><span class="line"><span class="params"><span class="function">        Device device)</span></span>;</span><br><span class="line"></span><br><span class="line">    ~<span class="built_in">PlacementDeleteContext</span>() &#123;</span><br><span class="line">        <span class="built_in">placement_dtor_</span>(data_ptr_.<span class="built_in">get</span>(), size_);</span><br><span class="line">        <span class="comment">// original memory will be freed when data_ptr_ is destructed</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
<p>其中：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> PlacementDtor = <span class="built_in"><span class="keyword">void</span></span> (*)(<span class="keyword">void</span>*, <span class="keyword">size_t</span>);</span><br></pre></td></tr></table></div></figure>
<p>即 <code>PlacementDtor</code> 是一个接受无类型指针以及 <code>size_t</code> 形参的无返回值的函数指针。</p>
<p>在构造函数中，<code>DataPtr</code> 接受的是右值引用，并且用移动语义 <code>std::move</code> 进行初始化，因此 <code>data_ptr</code> 是已经构建好的。在析构函数中，调用 <code>placement_dtor_</code> 并将数据指针的数据以及 <code>size_</code> 作为参数传递。</p>
<p>还有 <code>makeDataPtr</code> 的实现：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deletePlacementDeleteContext</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">delete</span> <span class="keyword">static_cast</span>&lt;PlacementDeleteContext*&gt;(ptr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">at::DataPtr <span class="title">PlacementDeleteContext::makeDataPtr</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    at::DataPtr&amp;&amp; data_ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">    PlacementDtor placement_dtor,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> size,</span></span></span><br><span class="line"><span class="params"><span class="function">    at::Device device)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span>* ptr = data_ptr.<span class="built_in">get</span>();</span><br><span class="line">  <span class="keyword">return</span> &#123;</span><br><span class="line">      ptr,</span><br><span class="line">      <span class="keyword">new</span> <span class="built_in">PlacementDeleteContext</span>(std::<span class="built_in">move</span>(data_ptr), placement_dtor, size),</span><br><span class="line">      &amp;deletePlacementDeleteContext,</span><br><span class="line">      device&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>回顾一下上面提到的 <code>DataPtr</code> 的构造函数，这里取出数据指针的数据，并构造一个 <code>PlacementDeleteContext</code> 作为上下文，使用 <code>deletePlacementDeleteContext</code> 作为删除器，还有平凡的 <code>device</code> 。</p>

        <h2 id="autogradmetainterface"   >
          <a href="#autogradmetainterface" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#autogradmetainterface"></a> AutogradMetaInterface</h2>
      
<p><code>AutogradMetaInterface</code> 类：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">AutogradMetaInterface</span> &#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">set_requires_grad</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> requires_grad,</span></span></span><br><span class="line"><span class="params"><span class="function">        at::TensorImpl* self_impl)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">requires_grad</span><span class="params">()</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> at::Tensor&amp; <span class="title">mutable_grad</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">const</span> at::Tensor&amp; <span class="title">grad</span><span class="params">()</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">const</span> at::Tensor&amp; <span class="title">fw_grad</span><span class="params">(<span class="keyword">uint64_t</span> level, <span class="keyword">const</span> at::TensorBase&amp; self)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">set_fw_grad</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> at::TensorBase&amp; new_grad,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> at::TensorBase&amp; self,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">uint64_t</span> level,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> is_inplace_op)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">AutogradMetaInterface</span>();</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
<p><code>AutogradMetaInterface</code> 类内全是抽象方法，可以去看一下继承它的 <a href="https://aurumting.cn/2021/12/05/PyTorch_autograd/"><code>AutogradMeta</code></a></p>

        <h2 id="pyinterpreter"   >
          <a href="#pyinterpreter" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#pyinterpreter"></a> PyInterpreter</h2>
      
<p>我们在 <code>TensorImpl</code> 上存储了一个 <code>PyObject</code> ，这样我们就可以有效地将张量翻译成 <code>Python</code> 表示法。然而，在某些情况下（<code>torchdeploy</code>），一个进程中可能有多个 <code>Python</code> 解释器，我们必须注意不要意外地将 <code>PyObjects</code> 与错误的解释器混淆。因此，我们也用它所对应的 <code>Python</code> 解释器来标记每个 <code>TensorImpl</code> 。</p>
<p>在 <code>torchdeploy</code> 中，我们有这些不变性：</p>
<ul>
<li>任何给定的 <code>TensorImpl</code> 最多可以与一个 <code>Python</code> 解释器相关联。我们将解释器标签表示为一个虚拟类实例的内存地址，每个解释器分配一次（这是为了在必要时，我们可以请求解释器为我们执行操作）</li>
<li>一个给定的 <code>TensorImpl</code> 的解释器标签只能从未初始化到被标记；一旦被标记，这就是一个静止状态（一旦被标记到一个解释器，就永远被标记到那个解释器）</li>
<li>当且仅当一个线程持有标记在 <code>TensorImpl</code> 上的解释器的 GIL 时，它可以改变 <code>TensorImpl</code> 的 <code>PyObject</code> 字段。（如果 <code>TensorImpl</code> 没有被标记，它必须首先原子化地要求它的标记，然后才能有效地写入）</li>
</ul>
<p><code>PyInterpreter</code> 对象本身是一个包含一些与解释器交互的函数指针的类。目前这只是为了调试，但如果一个 <code>Tensor</code> 可以拥有一个 <code>PyObject</code> ，解释器就可以用来释放它。</p>
<p>警告：这个类必须非常小心地编写，因为一个 <code>Tensor</code> 有可能引用一个对应于已经被卸载的共享库的解释器。这使得盲目地调用虚拟方法变得非常危险，因为 <code>vtable</code> 在那个时候可能是垃圾（在一个好的日子里，你可能得到 “纯虚拟方法被调用”）。</p>
<p>解决这个问题的想法是，我们总是泄露 <code>PyInterpreters</code>（所以即使在 <code>dlclose</code> 之后，它们也一直是活的），并通过用函数指针取代它们来解除 “虚拟方法”，而这些指针只是没有作用。这不能用传统的 C++ <code>vtable</code> 来做，所以我们必须推出我们自己的。</p>
<p>注意：将 <code>PyInterpreter</code> 标签表示为完整的对象的缺点是它在 <code>TensorImpl</code> 上需要一个额外的字。如果标签只是整数索引，在 64 位架构上，我们可以将标签和 <code>PyObject</code> 一起打包成一个原子字。在 32 位架构上，我们可以简单地说，只支持一个 <code>Python</code> 解释器 (如果试图设置一个非实质性的解释器标签，就会出错)。</p>
<p>这个方案的困难在于我们需要维护一个行外表来获取 <code>PyInterpreters</code> ，这样我们就可以对它们进行虚拟方法调用，而且对这个表的注册/解注册必须以线程安全的方式进行。如果可能的 <code>PyInterpreters</code> 的数量足够小（例如，8 位整数），这可以很容易做到，只需预先分配一个足够大的数组来容纳所有可能的解释器。 当然，128 个线程对任何人来说都是绰绰有余的！</p>
<p>我目前没有决定采用这种技术，因为 <code>PyInterpreter</code> 标签所增加的额外字数使我们的字数达到了 24 个，这意味着我们仍然适合在三条 8 字缓存线内。如果你需要分文不取，可以考虑这样做！</p>
<p><code>PyInterpreter</code> 类（删减版）：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">PyInterpreter</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">PyInterpreter</span> &#123;</span></span><br><span class="line">    <span class="keyword">using</span> name_sig = std::<span class="built_in">string</span>(<span class="keyword">const</span> PyInterpreter*);</span><br><span class="line">    <span class="keyword">using</span> decref_sig = <span class="built_in"><span class="keyword">void</span></span>(<span class="keyword">const</span> PyInterpreter*, PyObject*, <span class="keyword">bool</span>);</span><br><span class="line">    <span class="keyword">using</span> detach_sig =</span><br><span class="line">        c10::intrusive_ptr&lt;TensorImpl&gt;(<span class="keyword">const</span> PyInterpreter*, <span class="keyword">const</span> TensorImpl*);</span><br><span class="line">    <span class="keyword">using</span> dispatch_sig = <span class="built_in"><span class="keyword">void</span></span>(</span><br><span class="line">        <span class="keyword">const</span> PyInterpreter*,</span><br><span class="line">        <span class="keyword">const</span> c10::OperatorHandle&amp;,</span><br><span class="line">        torch::jit::Stack* stack,</span><br><span class="line">        <span class="keyword">const</span> std::shared_ptr&lt;TorchDispatchTypeObject&gt;&amp; type);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PyInterpreter</span>(</span><br><span class="line">        name_sig* name_fn,</span><br><span class="line">        decref_sig* decref_fn,</span><br><span class="line">        detach_sig* detach,</span><br><span class="line">        dispatch_sig* dispatch)</span><br><span class="line">        : <span class="built_in">name_fn_</span>(name_fn),</span><br><span class="line">            <span class="built_in">decref_fn_</span>(decref_fn),</span><br><span class="line">            <span class="built_in">detach_fn_</span>(detach),</span><br><span class="line">            <span class="built_in">dispatch_fn_</span>(dispatch) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    name_sig* name_fn_;</span><br><span class="line">    decref_sig* decref_fn_;</span><br><span class="line">    detach_sig* detach_fn_;</span><br><span class="line">    dispatch_sig* dispatch_fn_;</span><br><span class="line"></span><br><span class="line">    <span class="function">__ubsan_ignore_function__ std::string <span class="title">name</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (*name_fn_)(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="function">__ubsan_ignore_function__ <span class="keyword">void</span> <span class="title">decref</span><span class="params">(PyObject* pyobj, <span class="keyword">bool</span> is_tensor)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (*decref_fn_)(<span class="keyword">this</span>, pyobj, is_tensor);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">__ubsan_ignore_function__ c10::intrusive_ptr&lt;TensorImpl&gt; <span class="title">detach</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> TensorImpl* self)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (*detach_fn_)(<span class="keyword">this</span>, self);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke the Python boxed fallback dispatch to go back into Python</span></span><br><span class="line">    <span class="function">__ubsan_ignore_function__ <span class="keyword">void</span> <span class="title">dispatch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> c10::OperatorHandle&amp; op,</span></span></span><br><span class="line"><span class="params"><span class="function">        torch::jit::Stack* stack,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> std::shared_ptr&lt;TorchDispatchTypeObject&gt;&amp; type)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (*dispatch_fn_)(<span class="keyword">this</span>, op, stack, type);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">disarm</span><span class="params">()</span> <span class="keyword">noexcept</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>

        <h2 id="torchdispatchtypeobject"   >
          <a href="#torchdispatchtypeobject" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#torchdispatchtypeobject"></a> TorchDispatchTypeObject</h2>
      
<p>一个 <code>TorchDispatchTypeObject</code> 表示一个有 <code>__torch_dispatch__</code> 类方法的 <code>Tensor</code> 子类的类型。具体来说，它持有该类的 <code>PyObject*</code> 和 <code>PyInterpreter*</code> ，后者表示该类来自哪个 <code>python</code> 解释器。</p>
<p><code>TorchDispatchTypeObject</code> 类：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">TorchDispatchTypeObject</span> &#123;</span></span><br><span class="line">    <span class="comment">// Steals a reference to type_object</span></span><br><span class="line">    <span class="built_in">TorchDispatchTypeObject</span>(</span><br><span class="line">        PyObject* type_object,</span><br><span class="line">        c10::impl::PyInterpreter* pyinterpreter);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Releases the stolen reference to type_object</span></span><br><span class="line">    ~<span class="built_in">TorchDispatchTypeObject</span>();</span><br><span class="line"></span><br><span class="line">    c10::<span class="function">impl::PyInterpreter* <span class="title">pyinterpreter</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function">PyObject* <span class="title">ptr</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    PyObject* data_;</span><br><span class="line">    c10::impl::PyInterpreter* pyinterpreter_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>

        <h2 id="variableversion"   >
          <a href="#variableversion" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#variableversion"></a> VariableVersion</h2>
      
<p>每个张量都有一个版本计数器。每当张量的数据或大小通过原地变量操作发生变化时，版本计数器就会被递增。版本计数器用于检测对已保存变量的修改，这些修改会导致不正确的梯度计算。版本计数器可以在变量之间共享：</p>
<ol>
<li>视图共享基础变量的版本计数器</li>
<li><code>x.detach()</code> 共享 <code>x</code> 的版本计数器</li>
<li>未打包的保存变量共享源变量的版本计数器</li>
</ol>
<p>版本计数器在这些情况下是不共享的：</p>
<ol>
<li>当我们通过调用 <code>set_data(...)</code> 替换一个变量的底层张量时</li>
<li><code>x.data</code> 不共享 <code>x</code> 的版本计数器。</li>
</ol>
<p>为什么我们把版本计数器放在 <code>TensorImpl</code> 而不是 <code>AutogradMeta</code> 中？</p>
<p>回答：在 <code>Variable/Tensor</code> 合并后，当一个张量的 <code>requires_grad_</code> 为 <code>false</code> 时，它将没有 <code>AutogradMeta</code> ，但当我们在一个函数的前向传播中使用这个张量时，需要保存这个张量以备反向，我们需要跟踪这个张量的版本，以确保它在 <code>autograd</code> 图中始终有效。</p>
<p>为了实现这个目标，我们把版本计数器放在 <code>TensorImpl</code> 而不是 <code>AutogradMeta</code> 中，并让它始终可用。这允许我们在张量不需要梯度的时候有不携带 <code>AutogradMeta</code> 的优化。</p>
<p>实现这一目标的一个假设的替代方法是初始化 <code>AutogradMeta</code> ，并在不需要梯度的张量被保存为反向时才为其创建版本计数器。然而，由于为反向保存张量发生在前向传播中，而我们的不变式是前向传播需要是线程安全的，所以当我们在多线程情况下运行前向传播时，懒惰初始化 <code>AutogradMeta</code> 会引入竞争条件，从而使前向传递不再是线程安全的，这就破坏了不变式。</p>
<p><code>VariableVersion</code> 类（删减版）：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">VariableVersion</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">VersionCounter</span> :</span> intrusive_ptr_target &#123;</span><br><span class="line">        <span class="built_in">VersionCounter</span>(<span class="keyword">uint32_t</span> version) : <span class="built_in">version_</span>(version) &#123;&#125;</span><br><span class="line">        std::atomic&lt;<span class="keyword">uint32_t</span>&gt; version_;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    c10::intrusive_ptr&lt;VersionCounter&gt; version_counter_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">Disabled</span> &#123;</span> DISABLED &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">unique</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> version_counter_ ? <span class="number">1</span> == version_counter_.<span class="built_in">use_count</span>() : <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">VariableVersion</span>(<span class="keyword">uint32_t</span> version)</span><br><span class="line">        : <span class="built_in">version_counter_</span>(c10::make_intrusive&lt;VersionCounter&gt;(version)) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">enabled</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> version_counter_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">bump</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (version_counter_) &#123;</span><br><span class="line">            ++version_counter_-&gt;version_;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Inference tensor doesn&#x27;t have version counter so it shouldn&#x27;t be</span></span><br><span class="line">    <span class="comment">// accessed.</span></span><br><span class="line">    <span class="function"><span class="keyword">uint32_t</span> <span class="title">current_version</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> version_counter_-&gt;version_;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>

        <h2 id="tensorimpl-2"   >
          <a href="#tensorimpl-2" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#tensorimpl-2"></a> TensorImpl</h2>
      
<p>张量的底层表示，它包含一个指向存储（包含实际数据）和元数据（例如，大小和跨度）的指针，描述了数据作为张量的这个特定视图。</p>
<p>关于我们的张量的内存表示法的一些基本特征：</p>
<ul>
<li>
<p>它包含一个指向存储结构（<code>Storage/StorageImpl</code>）的指针，该结构包含指向实际数据的指针，并记录数据类型和视图的设备。这允许多个张量别名相同的底层数据，这允许在一个张量上有效地实现不同的 <em>视图</em>。</p>
</li>
<li>
<p>张量结构本身记录了关于张量的特定视图元数据，例如，大小、跨度和进入存储的偏移量。一个存储的每个视图可以有不同的大小或偏移。</p>
</li>
<li>
<p>这个类是侵入式的引用计数。它是引用计数的，所以我们可以支持大型张量的及时释放；它是侵入式的引用计数的，所以我们仍然可以对原始指针进行引用计数的操作，这在跨语言边界传递张量时往往更方便。</p>
</li>
<li>
<p>出于向后兼容的原因，一个张量可能处于未初始化的状态。一个张量可以通过以下两种方式被未初始化：</p>
<ul>
<li>张量可以是数据类型未初始化。这种形式的张量有一个未初始化的 <code>dtype</code> 。这种情况最常出现在用户写 <code>Tensor x(CPU)</code> 的时候。当第一次调用 <code>mutable_data&lt;T&gt;()</code> 时，<code>dtype</code> 和随后被初始化</li>
<li>一个张量可能是存储未初始化。这种形式的张量有非零的大小，但是有一个带有空数据指针的存储。这种情况最常出现在用户调用 <code>Resize()</code> 或 <code>FreeMemory()</code> 时。这是因为 <code>Caffe2</code> 在历史上进行了懒惰分配：在调用 <code>mutable_data&lt;T&gt;()</code> 之前，数据的分配不会发生。一个零大小的张量总是被存储初始化，因为在这种情况下不需要分配。</li>
</ul>
<p>这两种未初始化的状态的所有组合都是可能的。</p>
<p>张量上的所有其他字段总是被初始化。特别是，大小总是有效的。（历史上，一个声明为 <code>Tensor x(CPU)</code> 的张量也有未初始化的大小，编码为 <code>numel == -1</code> ，但我们现在决定默认为零大小，结果是 <code>numel == 0</code>）。</p>
<p>未初始化的存储空间必须是唯一拥有的，以保持我们的模型简单。因此，我们将拒绝那些可能导致未初始化的存储空间变成共享的操作（或者共享的存储空间变成未初始化的，例如来自 <code>FreeMemory</code>）。</p>
<p>在实践中，存储未初始化和数据类型未初始化的张量是 <em>极端</em> 短暂的：基本上，在你做了 <code>Resize()</code> 之后，你基本上总是在之后立即调用 <code>mutable_data()</code> 。如果给定一个存储未初始化的、数据类型未初始化的张量，大多数函数都不能工作。</p>
<p>我们打算消除所有未初始化的状态，因此每个张量在所有字段都是完全初始化的。请不要编写依赖这些未初始化状态的新代码。</p>
</li>
</ul>
<p><code>TensorImpl</code> 类（删减版）：</p>
<figure class="highlight cpp"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">TensorImpl</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line">    <span class="built_in">TensorImpl</span>() = <span class="keyword">delete</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">ImplType</span> &#123;</span> VIEW &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Construct a 1-dim 0-size tensor backed by the given storage.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="built_in">TensorImpl</span>(</span><br><span class="line">        Storage&amp;&amp; storage,</span><br><span class="line">        DispatchKeySet,</span><br><span class="line">        <span class="keyword">const</span> caffe2::TypeMeta data_type);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TensorImpl</span>(</span><br><span class="line">        ImplType,</span><br><span class="line">        Storage&amp;&amp; storage,</span><br><span class="line">        DispatchKeySet,</span><br><span class="line">        <span class="keyword">const</span> caffe2::TypeMeta data_type);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Construct a 1-dim 0 size tensor that doesn&#x27;t have a storage.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="built_in">TensorImpl</span>(</span><br><span class="line">        DispatchKeySet,</span><br><span class="line">        <span class="keyword">const</span> caffe2::TypeMeta data_type,</span><br><span class="line">        c10::optional&lt;c10::Device&gt; device_opt);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* other constructor */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">TensorImpl</span>(</span><br><span class="line">        Storage&amp;&amp; storage,</span><br><span class="line">        DispatchKeySet,</span><br><span class="line">        <span class="keyword">const</span> caffe2::TypeMeta data_type,</span><br><span class="line">        c10::optional&lt;c10::Device&gt;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">TensorImpl</span>(<span class="keyword">const</span> TensorImpl&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">    TensorImpl&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> TensorImpl&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">    <span class="built_in">TensorImpl</span>(TensorImpl&amp;&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">    TensorImpl&amp; <span class="keyword">operator</span>=(TensorImpl&amp;&amp;) = <span class="keyword">delete</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">release_resources</span><span class="params">()</span> <span class="keyword">override</span> </span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        autograd_meta_.<span class="built_in">reset</span>();</span><br><span class="line">        <span class="keyword">if</span> (storage_) &#123;</span><br><span class="line">            storage_ = &#123;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (owns_pyobj_) &#123;</span><br><span class="line">            <span class="built_in">TORCH_INTERNAL_ASSERT</span>(pyobj_interpreter_ != <span class="literal">nullptr</span>);</span><br><span class="line">            <span class="built_in">TORCH_INTERNAL_ASSERT</span>(pyobj_ != <span class="literal">nullptr</span>);</span><br><span class="line">            pyobj_interpreter_.<span class="built_in">load</span>(std::memory_order_acquire)</span><br><span class="line">                -&gt;<span class="built_in">decref</span>(pyobj_, <span class="comment">/*is_tensor*/</span> <span class="literal">true</span>);</span><br><span class="line">            pyobj_ = <span class="literal">nullptr</span>; <span class="comment">// for safety</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return the number of dimensions of this tensor.  Note that 0-dimension</span></span><br><span class="line"><span class="comment">    * represents a Tensor that is a Scalar, e.g., one that has a single element.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function">TENSORIMPL_MAYBE_VIRTUAL <span class="keyword">int64_t</span> <span class="title">dim</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    <span class="meta">#<span class="meta-keyword">ifdef</span> C10_DISABLE_TENSORIMPL_EXTENSIBILITY</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sizes_and_strides_.<span class="built_in">size</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">     ;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="function">TENSORIMPL_MAYBE_VIRTUAL <span class="keyword">bool</span> <span class="title">is_contiguous</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        at::MemoryFormat memory_format = at::MemoryFormat::Contiguous)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">C10_UNLIKELY</span>(</span><br><span class="line">                has_contiguity_ !=</span><br><span class="line">                <span class="keyword">static_cast</span>&lt;<span class="keyword">uint8_t</span>&gt;(HasContiguityPolicy::Default))) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">is_contiguous_nondefault_policy_impl</span>(memory_format);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">TORCH_INTERNAL_ASSERT_DEBUG_ONLY</span>(<span class="built_in">compute_contiguous</span>() == is_contiguous_);</span><br><span class="line">        <span class="keyword">if</span> (memory_format == at::MemoryFormat::ChannelsLast) &#123;</span><br><span class="line">            <span class="keyword">return</span> is_channels_last_contiguous_;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (memory_format == at::MemoryFormat::ChannelsLast3d) &#123;</span><br><span class="line">            <span class="keyword">return</span> is_channels_last_3d_contiguous_;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> is_contiguous_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">is_contiguous_nondefault_policy_impl</span><span class="params">(at::MemoryFormat)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">is_contiguous_custom</span><span class="params">(at::MemoryFormat memory_format)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">is_sparse</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> key_set_.<span class="built_in">has</span>(DispatchKey::SparseCPU) ||</span><br><span class="line">            key_set_.<span class="built_in">has</span>(DispatchKey::SparseCUDA) ||</span><br><span class="line">            key_set_.<span class="built_in">has</span>(DispatchKey::SparseHIP) ||</span><br><span class="line">            key_set_.<span class="built_in">has</span>(DispatchKey::SparseXPU);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* other is_xxx method */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Inference tensor doesn&#x27;t have autograd or ADInplaceOrView key.</span></span><br><span class="line">    <span class="comment">// Invariant:</span></span><br><span class="line">    <span class="comment">//   Inference tensor has version_counter_.enabled() == false</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">is_inference</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">bool</span> no_ADInplaceOrView = !key_set_.<span class="built_in">has</span>(c10::DispatchKey::ADInplaceOrView);</span><br><span class="line">        <span class="keyword">bool</span> no_Autograd = (key_set_ &amp; c10::autograd_dispatch_keyset).<span class="built_in">empty</span>();</span><br><span class="line">        <span class="keyword">return</span> no_ADInplaceOrView &amp;&amp; no_Autograd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Layout <span class="title">layout</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">is_sparse</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> kSparse;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">is_sparse_csr</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> kSparseCsr;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">is_mkldnn</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> kMkldnn;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> kStrided;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ~~~~~ Autograd API ~~~~~</span></span><br><span class="line">    <span class="comment">// Some methods below are defined in TensorImpl.cpp because Tensor is an</span></span><br><span class="line">    <span class="comment">// incomplete type.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Set whether or not a tensor requires gradient.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_requires_grad</span><span class="params">(<span class="keyword">bool</span> requires_grad)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * True if a tensor requires gradient.  Tensors which require gradient</span></span><br><span class="line"><span class="comment">    * have history tracked for any operations performed on them, so that</span></span><br><span class="line"><span class="comment">    * we can automatically differentiate back to them.  A tensor that</span></span><br><span class="line"><span class="comment">    * requires gradient and has no history is a &quot;leaf&quot; tensor, which we</span></span><br><span class="line"><span class="comment">    * accumulate gradients into.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">requires_grad</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Set whether or not to take the conjugate of the tensor (flip the imaginary</span></span><br><span class="line"><span class="comment">    * bit).</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">void</span> _set_conj(<span class="keyword">bool</span> value) &#123;</span><br><span class="line">        <span class="keyword">if</span> (value) &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">add</span>(DispatchKey::Conjugate);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">remove</span>(DispatchKey::Conjugate);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Set whether or not to take the conjugate of the tensor (flip the imaginary</span></span><br><span class="line"><span class="comment">    * bit).</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">void</span> _set_neg(<span class="keyword">bool</span> value) &#123;</span><br><span class="line">        <span class="keyword">if</span> (value) &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">add</span>(DispatchKey::Negative);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">remove</span>(DispatchKey::Negative);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return the accumulated gradient of a tensor. This gradient is computed</span></span><br><span class="line"><span class="comment">    * using forward mode AD.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This is an internal API that should never be used by end users.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * The API is as follows:</span></span><br><span class="line"><span class="comment">    *   - &quot;level&quot; allows to specify the level of forward AD nesting for which the</span></span><br><span class="line"><span class="comment">    *     gradient should be returned. Note that since levels are not fully</span></span><br><span class="line"><span class="comment">    *     supported yet, this argument should be 0. See documentation for</span></span><br><span class="line"><span class="comment">    *     torch::autograd::enter_dual_level for more details about forward AD</span></span><br><span class="line"><span class="comment">    * nesting.</span></span><br><span class="line"><span class="comment">    *   - &quot;self&quot; should represent the Tensor whose forward grad is accessed. It</span></span><br><span class="line"><span class="comment">    * is required when dealing with view.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">const</span> at::Tensor&amp; _fw_grad(<span class="keyword">uint64_t</span> level, <span class="keyword">const</span> at::TensorBase&amp; self) <span class="keyword">const</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Sets the forward gradient for this Tensor.</span></span><br><span class="line"><span class="comment">    * The given Tensor might not be used directly and its content will be copied.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">void</span> _set_fw_grad(</span><br><span class="line">        <span class="keyword">const</span> at::TensorBase&amp; new_grad,</span><br><span class="line">        <span class="keyword">const</span> at::TensorBase&amp; self,</span><br><span class="line">        <span class="keyword">uint64_t</span> level,</span><br><span class="line">        <span class="keyword">bool</span> is_inplace_op);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return a typed data pointer to the actual data which this tensor refers to.</span></span><br><span class="line"><span class="comment">    * This checks that the requested type (from the template parameter) matches</span></span><br><span class="line"><span class="comment">    * the internal type of the tensor.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> T* <span class="title">data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data_ptr_impl&lt;T&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * More efficient helper for Tensor::data_ptr(). Like data&lt;T&gt;(), but</span></span><br><span class="line"><span class="comment">    * does not do a type check. Unlike the untemplated data(), does</span></span><br><span class="line"><span class="comment">    * check has_storage() and storage_initialized().</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> T* <span class="title">data_ptr_impl</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Caller does the type check.</span></span><br><span class="line">        <span class="keyword">return</span> storage_.unsafe_data&lt;T&gt;() + storage_offset_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return a void* data pointer to the actual data which this tensor refers to.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * It is invalid to call data() on a dtype-uninitialized tensor, even if the</span></span><br><span class="line"><span class="comment">    * size is 0.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span>* <span class="title">data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">is_empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;<span class="keyword">void</span>*&gt;(</span><br><span class="line">            <span class="keyword">static_cast</span>&lt;<span class="keyword">char</span>*&gt;(storage_.<span class="built_in">data</span>()) +</span><br><span class="line">            data_type_.<span class="built_in">itemsize</span>() * storage_offset_);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Like data&lt;T&gt;(), but performs no checks.  You are responsible for ensuring</span></span><br><span class="line"><span class="comment">    * that all invariants required by data() are upheld here.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> T* <span class="title">unsafe_data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> storage_.unsafe_data&lt;T&gt;() + storage_offset_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">tensorimpl_type_name</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;TensorImpl&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    [[noreturn]] <span class="function"><span class="keyword">void</span> <span class="title">throw_storage_access_error</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * True if a tensor has no elements (e.g., numel() == 0).</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">is_empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">numel</span>() == <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Change the size at some dimension.  This DOES NOT update strides;</span></span><br><span class="line"><span class="comment">    * thus, most changes to size will not preserve contiguity.  You probably</span></span><br><span class="line"><span class="comment">    * also want to call set_stride() when you call this.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">set_size</span><span class="params">(<span class="keyword">int64_t</span> dim, <span class="keyword">int64_t</span> new_size)</span> </span>&#123;</span><br><span class="line">        sizes_and_strides_.<span class="built_in">size_at</span>(dim) = new_size;</span><br><span class="line">        <span class="built_in">refresh_numel</span>();</span><br><span class="line">        <span class="built_in">refresh_contiguous</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Change the stride at some dimension.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">set_stride</span><span class="params">(<span class="keyword">int64_t</span> dim, <span class="keyword">int64_t</span> new_stride)</span> </span>&#123;</span><br><span class="line">        sizes_and_strides_.<span class="built_in">stride_at_unchecked</span>(dim) = new_stride;</span><br><span class="line">        <span class="built_in">refresh_contiguous</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Like set_sizes_and_strides but assumes contiguous strides.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_sizes_contiguous</span><span class="params">(IntArrayRef new_size)</span> </span>&#123;</span><br><span class="line">        sizes_and_strides_.<span class="built_in">set_sizes</span>(new_size);</span><br><span class="line">        <span class="built_in">refresh_numel</span>();</span><br><span class="line">        <span class="built_in">empty_tensor_restride</span>(MemoryFormat::Contiguous);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Set the sizes and strides of a tensor.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_sizes_and_strides</span><span class="params">(IntArrayRef new_size, IntArrayRef new_stride)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span> new_dim = new_size.<span class="built_in">size</span>();</span><br><span class="line">        sizes_and_strides_.<span class="built_in">set_sizes</span>(new_size);</span><br><span class="line">        <span class="keyword">if</span> (new_dim &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> dim = new_dim - <span class="number">1</span>;; dim--) &#123;</span><br><span class="line">                <span class="keyword">if</span> (new_stride[dim] &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    sizes_and_strides_.<span class="built_in">stride_at_unchecked</span>(dim) = new_stride[dim];</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (dim == new_dim - <span class="number">1</span>) &#123;</span><br><span class="line">                        sizes_and_strides_.<span class="built_in">stride_at_unchecked</span>(dim) = <span class="number">1</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        sizes_and_strides_.<span class="built_in">stride_at_unchecked</span>(dim) =</span><br><span class="line">                            std::max&lt;<span class="keyword">int64_t</span>&gt;(</span><br><span class="line">                                sizes_and_strides_.<span class="built_in">size_at_unchecked</span>(dim + <span class="number">1</span>), <span class="number">1</span>) *</span><br><span class="line">                            sizes_and_strides_.<span class="built_in">stride_at_unchecked</span>(dim + <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (dim == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">refresh_numel</span>();</span><br><span class="line">        <span class="built_in">refresh_contiguous</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return the size of a tensor at some dimension.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">int64_t</span> <span class="title">size</span><span class="params">(<span class="keyword">int64_t</span> d)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return the stride of a tensor at some dimension.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">int64_t</span> <span class="title">stride</span><span class="params">(<span class="keyword">int64_t</span> d)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Set the pointer to autograd metadata.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_autograd_meta</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        std::unique_ptr&lt;c10::AutogradMetaInterface&gt; autograd_meta)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return the pointer to autograd metadata.  May return nullptr if the</span></span><br><span class="line"><span class="comment">    * tensor does not track gradients.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function">c10::AutogradMetaInterface* <span class="title">autograd_meta</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Set the pointer to named tensor metadata.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_named_tensor_meta</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        std::unique_ptr&lt;c10::NamedTensorMetaInterface&gt; named_tensor_meta)</span> </span>&#123;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">ifdef</span> DEBUG</span></span><br><span class="line">        <span class="keyword">if</span> (named_tensor_meta) &#123;</span><br><span class="line">            <span class="built_in">TORCH_INTERNAL_ASSERT</span>(named_tensor_meta-&gt;<span class="built_in">slow_dim</span>() == <span class="built_in">dim</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">        named_tensor_meta_ = std::<span class="built_in">move</span>(named_tensor_meta);</span><br><span class="line">        <span class="keyword">if</span> (named_tensor_meta_ == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">remove</span>(DispatchKey::Named);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">add</span>(DispatchKey::Named);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_python_dispatch</span><span class="params">(<span class="keyword">bool</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (k) &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">add</span>(DispatchKey::Python);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            key_set_ = key_set_.<span class="built_in">remove</span>(DispatchKey::Python);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return the pointer to named tensor metadata.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> c10::NamedTensorMetaInterface* <span class="title">named_tensor_meta</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> named_tensor_meta_.<span class="built_in">get</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">has_compatible_shallow_copy_type</span><span class="params">(DispatchKeySet from)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> is_dense = [](DispatchKeySet ts) &#123;</span><br><span class="line">        <span class="keyword">return</span> ts.<span class="built_in">has</span>(DispatchKey::CPU) || ts.<span class="built_in">has</span>(DispatchKey::CUDA) ||</span><br><span class="line">            ts.<span class="built_in">has</span>(DispatchKey::HIP) || ts.<span class="built_in">has</span>(DispatchKey::XPU);</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">auto</span> is_sparse = [](DispatchKeySet ts) &#123;</span><br><span class="line">        <span class="keyword">return</span> ts.<span class="built_in">has</span>(DispatchKey::SparseCPU) ||</span><br><span class="line">            ts.<span class="built_in">has</span>(DispatchKey::SparseCUDA) || ts.<span class="built_in">has</span>(DispatchKey::SparseHIP) ||</span><br><span class="line">            ts.<span class="built_in">has</span>(DispatchKey::SparseXPU);</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">return</span> (key_set_ == from) || (<span class="built_in">is_dense</span>(key_set_) &amp;&amp; <span class="built_in">is_dense</span>(from)) ||</span><br><span class="line">            (<span class="built_in">is_sparse</span>(key_set_) &amp;&amp; <span class="built_in">is_sparse</span>(from));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return a TensorImpl that is a shallow-copy of this TensorImpl.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> c10::intrusive_ptr&lt;TensorImpl&gt; <span class="title">shallow_copy_and_detach</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> c10::VariableVersion&amp; version_counter,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> allow_tensor_metadata_change)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Return a TensorImpl that is a shallow-copy of this TensorImpl.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> c10::intrusive_ptr&lt;TensorImpl&gt; <span class="title">shallow_copy_and_detach</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        c10::VariableVersion&amp;&amp; version_counter,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> allow_tensor_metadata_change)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Shallow-copies data from another TensorImpl into this TensorImpl.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">shallow_copy_from</span><span class="params">(<span class="keyword">const</span> c10::intrusive_ptr&lt;TensorImpl&gt;&amp; impl)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">copy_tensor_metadata</span>(</span><br><span class="line">            <span class="comment">/*src_impl=*/</span>impl.<span class="built_in">get</span>(),</span><br><span class="line">            <span class="comment">/*dest_impl=*/</span><span class="keyword">this</span>,</span><br><span class="line">            <span class="comment">/*version_counter=*/</span><span class="built_in">version_counter</span>(),</span><br><span class="line">            <span class="comment">/*allow_tensor_metadata_change=*/</span><span class="built_in">allow_tensor_metadata_change</span>());</span><br><span class="line">        <span class="built_in">refresh_numel</span>();</span><br><span class="line">        <span class="built_in">refresh_contiguous</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Query the PyObject interpreter.  This may return null if there is no</span></span><br><span class="line">    <span class="comment">// interpreter.  This is racy!</span></span><br><span class="line">    <span class="function">impl::PyInterpreter* <span class="title">pyobj_interpreter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pyobj_interpreter_.<span class="built_in">load</span>(std::memory_order_acquire);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function">c10::optional&lt;c10::Device&gt; <span class="title">device_opt</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> device_opt_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * @brief Extends the outer-most dimension of this tensor by num elements,</span></span><br><span class="line"><span class="comment">    * preserving the existing data.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * The underlying data may be reallocated in order to accommodate the new</span></span><br><span class="line"><span class="comment">    * elements, in which case this tensors&#x27; capacity is grown at a factor of</span></span><br><span class="line"><span class="comment">    * growthPct. This ensures that Extend runs on an amortized O(1) time</span></span><br><span class="line"><span class="comment">    * complexity.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This op is auto-asynchronous if the underlying device (CUDA) supports it.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Extend</span><span class="params">(<span class="keyword">int64_t</span> num, <span class="keyword">float</span> growthPct)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">using</span> SizesVector = SmallVector&lt;<span class="keyword">int64_t</span>, <span class="number">5</span>&gt;;</span><br><span class="line">        <span class="function">SizesVector <span class="title">newDims</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            sizes_and_strides_.sizes_begin(), sizes_and_strides_.sizes_end())</span></span>;</span><br><span class="line">        newDims[<span class="number">0</span>] += num;</span><br><span class="line">        <span class="keyword">if</span> (!storage_.<span class="built_in">data</span>()) &#123;</span><br><span class="line">            <span class="built_in">Resize</span>(newDims);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span> newNumel =</span><br><span class="line">            c10::<span class="built_in">multiply_integers</span>(newDims.<span class="built_in">begin</span>(), newDims.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">if</span> (newNumel * data_type_.<span class="built_in">itemsize</span>() &lt;= storage_.<span class="built_in">nbytes</span>()) &#123;</span><br><span class="line">            sizes_and_strides_.<span class="built_in">set_sizes</span>(newDims);</span><br><span class="line">            numel_ = newNumel;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function">SizesVector <span class="title">newCapacity</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            sizes_and_strides_.sizes_begin(), sizes_and_strides_.sizes_end())</span></span>;</span><br><span class="line">        newCapacity[<span class="number">0</span>] = std::<span class="built_in">max</span>(</span><br><span class="line">            newDims[<span class="number">0</span>],</span><br><span class="line">            <span class="keyword">static_cast</span>&lt;<span class="keyword">int64_t</span>&gt;(std::<span class="built_in">ceil</span>(</span><br><span class="line">                sizes_and_strides_.<span class="built_in">size_at_unchecked</span>(<span class="number">0</span>) * (<span class="number">1</span> + growthPct / <span class="number">100</span>))));</span><br><span class="line">        <span class="keyword">auto</span> oldData = std::<span class="built_in">move</span>(storage_.<span class="built_in">data_ptr</span>());</span><br><span class="line">        <span class="keyword">auto</span> oldSize = numel_;</span><br><span class="line">        <span class="built_in">Resize</span>(newCapacity);</span><br><span class="line">        <span class="keyword">auto</span>* newData = <span class="built_in">raw_mutable_data</span>(data_type_);</span><br><span class="line">        <span class="keyword">if</span> (data_type_.<span class="built_in">copy</span>()) &#123;</span><br><span class="line">            data_type_.<span class="built_in">copy</span>()(oldData.<span class="built_in">get</span>(), newData, oldSize);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">CopyBytes</span>(</span><br><span class="line">                oldSize * <span class="built_in">itemsize</span>(),</span><br><span class="line">                oldData.<span class="built_in">get</span>(),</span><br><span class="line">                <span class="built_in">device</span>(),</span><br><span class="line">                newData,</span><br><span class="line">                <span class="built_in">device</span>(),</span><br><span class="line">                <span class="literal">true</span>); <span class="comment">// non-blocking</span></span><br><span class="line">        &#125;</span><br><span class="line">        reserved_ = <span class="literal">true</span>;</span><br><span class="line">        sizes_and_strides_.<span class="built_in">set_sizes</span>(newDims);</span><br><span class="line">        numel_ = newNumel;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * @brief Reserve space for the underlying tensor.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This must be called after Resize(), since we only specify the first</span></span><br><span class="line"><span class="comment">    * dimension This does not copy over the old data to the newly allocated space</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ReserveSpace</span><span class="params">(<span class="keyword">const</span> T&amp; outer_dim)</span> </span>&#123;</span><br><span class="line">        <span class="function">SmallVector&lt;<span class="keyword">int64_t</span>, 5&gt; <span class="title">newCapacity</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            sizes_and_strides_.sizes_begin(), sizes_and_strides_.sizes_end())</span></span>;</span><br><span class="line">        newCapacity[<span class="number">0</span>] = outer_dim;</span><br><span class="line">        <span class="keyword">auto</span> newNumel = c10::<span class="built_in">multiply_integers</span>(newCapacity);</span><br><span class="line">        <span class="keyword">if</span> (newNumel * data_type_.<span class="built_in">itemsize</span>() &lt;= storage_.<span class="built_in">nbytes</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        storage_.<span class="built_in">data_ptr</span>().<span class="built_in">clear</span>();</span><br><span class="line">        <span class="keyword">auto</span> oldSize = numel_;</span><br><span class="line">        <span class="function">SmallVector&lt;<span class="keyword">int64_t</span>, 5&gt; <span class="title">oldDims</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            sizes_and_strides_.sizes_begin(), sizes_and_strides_.sizes_end())</span></span>;</span><br><span class="line">        <span class="built_in">Resize</span>(newCapacity);</span><br><span class="line">        <span class="built_in">raw_mutable_data</span>(data_type_);</span><br><span class="line">        sizes_and_strides_.<span class="built_in">set_sizes</span>(oldDims);</span><br><span class="line">        numel_ = oldSize;</span><br><span class="line">        reserved_ = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * @brief Resizes a tensor.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * Resize takes in a vector of ints specifying the dimensions of the tensor.</span></span><br><span class="line"><span class="comment">    * You can pass in an empty vector to specify that it is a scalar (i.e.</span></span><br><span class="line"><span class="comment">    * containing one single item).</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * The underlying storage may be deleted after calling Resize: if the new</span></span><br><span class="line"><span class="comment">    * shape leads to a different number of items in the tensor, the old memory</span></span><br><span class="line"><span class="comment">    * is deleted and new memory will be allocated next time you call</span></span><br><span class="line"><span class="comment">    * mutable_data(). However, if the shape is different but the total number of</span></span><br><span class="line"><span class="comment">    * items is the same, the underlying storage is kept.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * This method respects caffe2_keep_on_shrink.  Consult the internal logic</span></span><br><span class="line"><span class="comment">    * of this method to see exactly under what circumstances this flag matters.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Resize</span><span class="params">(Ts... dim_source)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">bool</span> size_changed = <span class="built_in">SetDims</span>(dim_source...);</span><br><span class="line">        <span class="keyword">if</span> (size_changed) &#123;</span><br><span class="line">            <span class="built_in">HandleResize</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Resizes the tensor without touching underlying storage.</span></span><br><span class="line"><span class="comment">    * This requires the total size of the tensor to remains constant.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> std::vector&lt;<span class="keyword">int64_t</span>&gt;&amp; dims)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int64_t</span> new_size = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> d : dims) &#123;</span><br><span class="line">            <span class="built_in">TORCH_CHECK</span>(d &gt;= <span class="number">0</span>);</span><br><span class="line">            new_size *= d;</span><br><span class="line">        &#125;</span><br><span class="line">        sizes_and_strides_.<span class="built_in">set_sizes</span>(dims);</span><br><span class="line">        <span class="built_in">empty_tensor_restride</span>(MemoryFormat::Contiguous);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Release whatever memory the tensor was holding but keep size and type</span></span><br><span class="line"><span class="comment">    * information. Subsequent call to mutable_data will trigger new memory</span></span><br><span class="line"><span class="comment">    * allocation.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">FreeMemory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        storage_ = Storage::<span class="built_in">create_legacy</span>(storage_.<span class="built_in">device</span>());</span><br><span class="line">        storage_offset_ = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * @brief Shares the data with another tensor.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * To share data between two tensors, the sizes of the two tensors must be</span></span><br><span class="line"><span class="comment">    * equal already. The reason we do not implicitly do a Resize to make the two</span></span><br><span class="line"><span class="comment">    * tensors have the same shape is that we want to allow tensors of different</span></span><br><span class="line"><span class="comment">    * shapes but the same number of items to still be able to share data. This</span></span><br><span class="line"><span class="comment">    * allows one to e.g. have a n-dimensional Tensor and a flattened version</span></span><br><span class="line"><span class="comment">    * sharing the same underlying storage.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * The source tensor should already have its data allocated.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="comment">// To be deprecated</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ShareData</span><span class="params">(<span class="keyword">const</span> TensorImpl&amp; src)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// It is possible that the source tensor hasn&#x27;t called mutable_data() yet,</span></span><br><span class="line">        <span class="comment">// in which case ShareData() doesn&#x27;t make much sense since we don&#x27;t really</span></span><br><span class="line">        <span class="comment">// know what to share yet.</span></span><br><span class="line">        <span class="keyword">if</span> (!src.<span class="built_in">dtype_initialized</span>()) &#123;</span><br><span class="line">        <span class="built_in">C10_LOG_EVERY_MS</span>(WARNING, <span class="number">1000</span>)</span><br><span class="line">            &lt;&lt; <span class="string">&quot;Source tensor don&#x27;t have a data type (did you call mutable_data&lt;T&gt; on the tensor?)&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Finally, do sharing.</span></span><br><span class="line">        <span class="comment">/* Since we create new Storage whenever we need to change data_type/nbytes</span></span><br><span class="line"><span class="comment">        * this still keeps the original semantics</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        storage_ = src.<span class="built_in">storage</span>();</span><br><span class="line">        data_type_ = src.<span class="built_in">dtype</span>();</span><br><span class="line">        device_opt_ = src.<span class="built_in">device_opt</span>();</span><br><span class="line">        storage_offset_ = src.<span class="built_in">storage_offset</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ShareExternalPointer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        DataPtr&amp;&amp; data_ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> caffe2::TypeMeta data_type,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">size_t</span> size_bytes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!size_bytes) &#123;</span><br><span class="line">            size_bytes = numel_ * data_type.<span class="built_in">itemsize</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (storage_.<span class="built_in">unique</span>()) &#123;</span><br><span class="line">            storage_.<span class="built_in">UniqueStorageShareExternalPointer</span>(</span><br><span class="line">                std::<span class="built_in">move</span>(data_ptr), size_bytes);</span><br><span class="line">            data_type_ = data_type;</span><br><span class="line">            device_opt_ = storage_.<span class="built_in">device</span>();</span><br><span class="line">            storage_offset_ = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// Create a new Storage</span></span><br><span class="line">            storage_ = <span class="built_in">Storage</span>(</span><br><span class="line">                Storage::<span class="built_in">use_byte_size_t</span>(),</span><br><span class="line">                size_bytes,</span><br><span class="line">                std::<span class="built_in">move</span>(data_ptr),</span><br><span class="line">                <span class="comment">/*allocator=*/</span><span class="literal">nullptr</span>,</span><br><span class="line">                <span class="comment">/*resizable=*/</span><span class="literal">false</span>);</span><br><span class="line">            data_type_ = data_type;</span><br><span class="line">            device_opt_ = storage_.<span class="built_in">device</span>();</span><br><span class="line">            storage_offset_ = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a mutable raw pointer of the underlying storage. Since we will need</span></span><br><span class="line"><span class="comment">    * to know the type of the data for allocation, a TypeMeta object is passed in</span></span><br><span class="line"><span class="comment">    * to specify the necessary information. This is conceptually equivalent of</span></span><br><span class="line"><span class="comment">    * calling mutable_data&lt;T&gt;() where the TypeMeta parameter meta is derived from</span></span><br><span class="line"><span class="comment">    * the type T. This function differs from mutable_data&lt;T&gt;() in the sense that</span></span><br><span class="line"><span class="comment">    * the type T can be specified during runtime via the TypeMeta object.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * If the existing data does not match the desired type, it will be deleted</span></span><br><span class="line"><span class="comment">    * and a new storage will be created.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span>* <span class="title">raw_mutable_data</span><span class="params">(<span class="keyword">const</span> caffe2::TypeMeta meta)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// For 0-size tensors it&#x27;s fine to return any pointer (including nullptr)</span></span><br><span class="line">        <span class="keyword">if</span> (data_type_ == meta &amp;&amp; <span class="built_in">storage_initialized</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;<span class="keyword">void</span>*&gt;(</span><br><span class="line">                <span class="keyword">static_cast</span>&lt;<span class="keyword">char</span>*&gt;(storage_.<span class="built_in">data</span>()) +</span><br><span class="line">                storage_offset_ * meta.<span class="built_in">itemsize</span>());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">bool</span> had_special_dtor = data_type_.<span class="built_in">placementDelete</span>() != <span class="literal">nullptr</span>;</span><br><span class="line">            storage_offset_ = <span class="number">0</span>;</span><br><span class="line">            data_type_ = meta;</span><br><span class="line">            <span class="comment">// We can reuse the existing buffer if the current data does not have</span></span><br><span class="line">            <span class="comment">// a special destructor and the new data doesn&#x27;t have a special</span></span><br><span class="line">            <span class="comment">// constructor.</span></span><br><span class="line">            <span class="keyword">if</span> (numel_ == <span class="number">0</span> ||</span><br><span class="line">                (meta.<span class="built_in">placementNew</span>() == <span class="literal">nullptr</span> &amp;&amp; !had_special_dtor &amp;&amp;</span><br><span class="line">                (storage_.<span class="built_in">nbytes</span>() &gt;= (numel_ * data_type_.<span class="built_in">itemsize</span>())))) &#123;</span><br><span class="line">                <span class="built_in">TORCH_INTERNAL_ASSERT</span>(</span><br><span class="line">                    storage_offset_ == <span class="number">0</span>); <span class="comment">// because we just reallocated</span></span><br><span class="line">                <span class="keyword">return</span> storage_.<span class="built_in">data</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">const</span> Allocator* allocator = storage_.<span class="built_in">allocator</span>();</span><br><span class="line">            <span class="keyword">if</span> (allocator == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                allocator = <span class="built_in">GetAllocator</span>(storage_.<span class="built_in">device_type</span>());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (meta.<span class="built_in">placementNew</span>()) &#123;</span><br><span class="line">                <span class="keyword">auto</span> size = numel_;</span><br><span class="line">                <span class="keyword">auto</span> dtor = data_type_.<span class="built_in">placementDelete</span>();</span><br><span class="line">                <span class="keyword">auto</span> data_ptr = allocator-&gt;<span class="built_in">allocate</span>(numel_ * data_type_.<span class="built_in">itemsize</span>());</span><br><span class="line">                storage_.<span class="built_in">set_data_ptr_noswap</span>(PlacementDeleteContext::<span class="built_in">makeDataPtr</span>(</span><br><span class="line">                    std::<span class="built_in">move</span>(data_ptr), dtor, size, storage_.<span class="built_in">device</span>()));</span><br><span class="line">                data_type_.<span class="built_in">placementNew</span>()(storage_.<span class="built_in">data</span>(), numel_);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                storage_.<span class="built_in">set_data_ptr_noswap</span>(</span><br><span class="line">                    allocator-&gt;<span class="built_in">allocate</span>(numel_ * data_type_.<span class="built_in">itemsize</span>()));</span><br><span class="line">            &#125;</span><br><span class="line">            storage_.<span class="built_in">set_nbytes</span>(numel_ * data_type_.<span class="built_in">itemsize</span>());</span><br><span class="line">            <span class="built_in">TORCH_INTERNAL_ASSERT</span>(</span><br><span class="line">                storage_offset_ == <span class="number">0</span>); <span class="comment">// because we just reallocated</span></span><br><span class="line">            device_opt_ = storage_.<span class="built_in">device</span>();</span><br><span class="line">            <span class="keyword">return</span> storage_.<span class="built_in">data</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns a typed pointer of the underlying storage.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * For fundamental types, we reuse possible existing storage if there</span></span><br><span class="line"><span class="comment">    * is sufficient capacity.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> T* <span class="title">mutable_data</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">storage_initialized</span>() &amp;&amp; data_type_.Match&lt;T&gt;()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;T*&gt;(storage_.<span class="built_in">data</span>()) + storage_offset_;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;T*&gt;(<span class="built_in">raw_mutable_data</span>(caffe2::TypeMeta::Make&lt;T&gt;()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * True if a tensor is storage initialized.  A tensor may become</span></span><br><span class="line"><span class="comment">    * storage UNINITIALIZED after a Resize() or FreeMemory()</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">storage_initialized</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> storage_.<span class="built_in">data</span>() || numel_ == <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * True if a tensor is dtype initialized.  A tensor allocated with</span></span><br><span class="line"><span class="comment">    * Caffe2-style constructors is dtype uninitialized until the</span></span><br><span class="line"><span class="comment">    * first time mutable_data&lt;T&gt;() is called.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">dtype_initialized</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data_type_ != caffe2::<span class="built_in">TypeMeta</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_storage_keep_dtype</span><span class="params">(at::Storage storage)</span> </span>&#123;</span><br><span class="line">        storage_ = std::<span class="built_in">move</span>(storage);</span><br><span class="line">        device_opt_ = storage_.<span class="built_in">device</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_storage_and_dtype</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        at::Storage storage,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> caffe2::TypeMeta data_type)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">set_storage_keep_dtype</span>(storage);</span><br><span class="line">        data_type_ = data_type;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">HandleResize</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The Caffe2 Resize() method supports being called both as Resize(&#123;2,2&#125;) as</span></span><br><span class="line">    <span class="comment">// well as variadic with Resize(2, 2).  These overloads provide all of the</span></span><br><span class="line">    <span class="comment">// supported calling configurations, while being overloads (and not templates)</span></span><br><span class="line">    <span class="comment">// so that implicit conversions still work.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// SetDims on ArrayRef is internally implemented as a template, so we can</span></span><br><span class="line">    <span class="comment">// handle both ArrayRefs of different types (there are some uses of</span></span><br><span class="line">    <span class="comment">// Resize in Caffe2 which pass in int, not int64_t.)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;</span><br><span class="line">        <span class="keyword">typename</span> T,</span><br><span class="line">        <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_integral&lt;T&gt;::value&gt;::type&gt;</span><br><span class="line">    <span class="keyword">bool</span> <span class="built_in">SetDimsTemplate</span>(ArrayRef&lt;T&gt; src) &#123;</span><br><span class="line">        <span class="keyword">auto</span> old_numel = numel_;</span><br><span class="line">        sizes_and_strides_.<span class="built_in">resize</span>(src.<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">int64_t</span> new_numel = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; src.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">            new_numel *= src[i];</span><br><span class="line">            sizes_and_strides_.<span class="built_in">size_at_unchecked</span>(i) = src[i];</span><br><span class="line">        &#125;</span><br><span class="line">        numel_ = new_numel;</span><br><span class="line">        <span class="built_in">empty_tensor_restride</span>(MemoryFormat::Contiguous);</span><br><span class="line">        <span class="keyword">return</span> numel_ != old_numel;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Compute the number of elements based on the sizes of a tensor.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">int64_t</span> <span class="title">compute_numel</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int64_t</span> n = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> s : <span class="built_in">sizes</span>()) &#123;</span><br><span class="line">            n *= s;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Compute the number of elements based on the sizes of a</span></span><br><span class="line"><span class="comment">    * tensor. Catches integer overflow that may occur when a tensor</span></span><br><span class="line"><span class="comment">    * using a sparse layout has multiple dimensions with large sizes.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">int64_t</span> <span class="title">safe_compute_numel</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int64_t</span> n = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> s : <span class="built_in">sizes</span>()) &#123;</span><br><span class="line">            n *= s;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Compute whether or not a tensor is contiguous based on the sizes and</span></span><br><span class="line"><span class="comment">    * strides of a tensor.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">compute_contiguous</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">compute_channels_last_contiguous_2d</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">compute_channels_last_contiguous_3d</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">compute_strides_like_channels_last_2d</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">compute_strides_like_channels_last_3d</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">compute_non_overlapping_and_dense</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">refresh_numel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        numel_ = <span class="built_in">compute_numel</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">safe_refresh_numel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        numel_ = <span class="built_in">safe_compute_numel</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Recompute the cached contiguity of a tensor.  Call this if you modify sizes</span></span><br><span class="line"><span class="comment">    * or strides.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">refresh_contiguous</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        is_contiguous_ = <span class="built_in">compute_contiguous</span>();</span><br><span class="line">        <span class="built_in"><span class="keyword">switch</span></span> (<span class="built_in">dim</span>()) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">            is_channels_last_contiguous_ = <span class="built_in">compute_channels_last_contiguous_2d</span>();</span><br><span class="line">            is_channels_last_3d_contiguous_ = <span class="literal">false</span>;</span><br><span class="line">            is_channels_last_ = <span class="built_in">compute_strides_like_channels_last_2d</span>();</span><br><span class="line">            is_channels_last_3d_ = <span class="literal">false</span>;</span><br><span class="line">            is_non_overlapping_and_dense_ = is_contiguous_ ||</span><br><span class="line">                is_channels_last_contiguous_ || <span class="built_in">compute_non_overlapping_and_dense</span>();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">            is_channels_last_contiguous_ = <span class="built_in">compute_channels_last_contiguous_2d</span>();</span><br><span class="line">            is_channels_last_3d_contiguous_ = !is_channels_last_contiguous_ &amp;&amp;</span><br><span class="line">                <span class="built_in">compute_channels_last_contiguous_3d</span>();</span><br><span class="line">            is_channels_last_ = !is_channels_last_3d_contiguous_ &amp;&amp;</span><br><span class="line">                <span class="built_in">compute_strides_like_channels_last_2d</span>();</span><br><span class="line">            is_channels_last_3d_ =</span><br><span class="line">                !is_channels_last_ &amp;&amp; <span class="built_in">compute_strides_like_channels_last_3d</span>();</span><br><span class="line">            is_non_overlapping_and_dense_ = is_contiguous_ ||</span><br><span class="line">                is_channels_last_contiguous_ || is_channels_last_3d_contiguous_ ||</span><br><span class="line">                <span class="built_in">compute_non_overlapping_and_dense</span>();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            is_channels_last_contiguous_ = <span class="literal">false</span>;</span><br><span class="line">            is_channels_last_3d_contiguous_ = <span class="literal">false</span>;</span><br><span class="line">            is_channels_last_ = <span class="literal">false</span>;</span><br><span class="line">            is_channels_last_3d_ = <span class="literal">false</span>;</span><br><span class="line">            is_non_overlapping_and_dense_ =</span><br><span class="line">                is_contiguous_ || <span class="built_in">compute_non_overlapping_and_dense</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Copy the tensor metadata fields (e.g. sizes / strides / storage pointer /</span></span><br><span class="line"><span class="comment">    * storage_offset) from one TensorImpl to another TensorImpl.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copy_tensor_metadata</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> TensorImpl* src_impl,</span></span></span><br><span class="line"><span class="params"><span class="function">        TensorImpl* dest_impl,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> c10::VariableVersion&amp; version_counter,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> allow_tensor_metadata_change)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Copy the tensor metadata fields (e.g. sizes / strides / storage pointer /</span></span><br><span class="line"><span class="comment">    * storage_offset) from one TensorImpl to another TensorImpl.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copy_tensor_metadata</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> TensorImpl* src_impl,</span></span></span><br><span class="line"><span class="params"><span class="function">        TensorImpl* dest_impl,</span></span></span><br><span class="line"><span class="params"><span class="function">        c10::VariableVersion&amp;&amp; version_counter,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> allow_tensor_metadata_change)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copy_tensor_metadata_except_version_counter</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">const</span> TensorImpl* src_impl,</span></span></span><br><span class="line"><span class="params"><span class="function">        TensorImpl* dest_impl,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">bool</span> allow_tensor_metadata_change)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="comment">// Error message to show when the user tries to change tensor metadata on</span></span><br><span class="line">    <span class="comment">// Tensor created from .data or .detach().</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="keyword">const</span> err_msg_tensor_metadata_change_not_allowed;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="comment">// Policy for adjusting the behavior of is_contiguous(). Allows</span></span><br><span class="line">    <span class="comment">// subclass customization while still being able to inline</span></span><br><span class="line">    <span class="comment">// is_contiguous() in the common case.</span></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="keyword">class</span> <span class="title">HasContiguityPolicy</span> :</span> <span class="keyword">uint8_t</span> &#123;</span><br><span class="line">        <span class="comment">// Default behavior: check is_contiguous_ and similar bitflags.</span></span><br><span class="line">        Default,</span><br><span class="line">        <span class="comment">// Throw a generic error message that this tensor type does not</span></span><br><span class="line">        <span class="comment">// support is_contiguous.</span></span><br><span class="line">        ContiguityNotSupported,</span><br><span class="line">        <span class="comment">// Call virtual is_contiguous_custom method to implement custom</span></span><br><span class="line">        <span class="comment">// is_contiguous behavior.</span></span><br><span class="line">        CustomBehavior,</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_has_contiguity_policy</span><span class="params">(HasContiguityPolicy p)</span> </span>&#123;</span><br><span class="line">        has_contiguity_ = <span class="keyword">static_cast</span>&lt;<span class="keyword">uint8_t</span>&gt;(p);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Storage storage_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::unique_ptr&lt;c10::AutogradMetaInterface&gt; autograd_meta_ = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    std::unique_ptr&lt;c10::NamedTensorMetaInterface&gt; named_tensor_meta_ = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">    c10::VariableVersion version_counter_;</span><br><span class="line"> </span><br><span class="line">    std::atomic&lt;impl::PyInterpreter*&gt; pyobj_interpreter_;</span><br><span class="line"></span><br><span class="line">    PyObject* pyobj_;</span><br><span class="line"></span><br><span class="line">    c10::impl::SizesAndStrides sizes_and_strides_;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int64_t</span> storage_offset_ = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">int64_t</span> numel_ = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    caffe2::TypeMeta data_type_;</span><br><span class="line"></span><br><span class="line">    c10::optional&lt;c10::Device&gt; device_opt_;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Tensor is contiguous</span></span><br><span class="line">    <span class="keyword">bool</span> is_contiguous_ : <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// gcc doesn&#x27;t like enum class bitfields; see</span></span><br><span class="line">    <span class="comment">// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61414</span></span><br><span class="line">    <span class="comment">/* HasContiguityPolicy */</span> <span class="keyword">uint8_t</span> has_contiguity_ : <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Tensor is a subclass that does not permit storage access.</span></span><br><span class="line">    <span class="keyword">bool</span> storage_access_should_throw_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// default member initializers for bit-fields only available with -std=c++2a</span></span><br><span class="line">    <span class="comment">// or -std=gnu++2a</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">init_bitfields</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        is_contiguous_ = <span class="literal">true</span>;</span><br><span class="line">        has_contiguity_ = <span class="keyword">static_cast</span>&lt;<span class="keyword">uint8_t</span>&gt;(HasContiguityPolicy::Default);</span><br><span class="line">        is_channels_last_ = <span class="literal">false</span>;</span><br><span class="line">        is_channels_last_contiguous_ = <span class="literal">false</span>;</span><br><span class="line">        is_channels_last_3d_ = <span class="literal">false</span>;</span><br><span class="line">        is_channels_last_3d_contiguous_ = <span class="literal">false</span>;</span><br><span class="line">        is_non_overlapping_and_dense_ = <span class="literal">true</span>;</span><br><span class="line">        is_wrapped_number_ = <span class="literal">false</span>;</span><br><span class="line">        allow_tensor_metadata_change_ = <span class="literal">true</span>;</span><br><span class="line">        reserved_ = <span class="literal">false</span>;</span><br><span class="line">        owns_pyobj_ = <span class="literal">false</span>;</span><br><span class="line">        storage_access_should_throw_ = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> is_channels_last_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> is_channels_last_contiguous_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> is_channels_last_3d_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> is_channels_last_3d_contiguous_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> is_non_overlapping_and_dense_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> is_wrapped_number_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> allow_tensor_metadata_change_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> reserved_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> owns_pyobj_ : <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    DispatchKeySet key_set_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></div></figure>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://aurumting.cn">Aurumting</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://aurumting.cn/2021/12/04/PyTorch_c10/">https://aurumting.cn/2021/12/04/PyTorch_c10/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://aurumting.cn/tags/PyTorch/">PyTorch</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/12/07/PyTorch_intrusive_ptr/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">PyTorch 源码阅读：intrusive_ptr</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/11/28/automatic-differential/"><span class="paginator-prev__text">自动微分</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#uniquevoidptr"><span class="toc-number">1.</span> <span class="toc-text">
           UniqueVoidPtr</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#allocator"><span class="toc-number">2.</span> <span class="toc-text">
           Allocator</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#storage"><span class="toc-number">3.</span> <span class="toc-text">
           Storage</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tensorimpl"><span class="toc-number">4.</span> <span class="toc-text">
           TensorImpl</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#placementdeletecontext"><span class="toc-number">4.1.</span> <span class="toc-text">
           PlacementDeleteContext</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#autogradmetainterface"><span class="toc-number">4.2.</span> <span class="toc-text">
           AutogradMetaInterface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pyinterpreter"><span class="toc-number">4.3.</span> <span class="toc-text">
           PyInterpreter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchdispatchtypeobject"><span class="toc-number">4.4.</span> <span class="toc-text">
           TorchDispatchTypeObject</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#variableversion"><span class="toc-number">4.5.</span> <span class="toc-text">
           VariableVersion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorimpl-2"><span class="toc-number">4.6.</span> <span class="toc-text">
           TensorImpl</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/avatar.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">雄城壮看江山无恙， 谁识我一蓑一笠到天涯</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/Aurainting" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://blog.csdn.net/qq_46013251" target="_blank" rel="noopener" data-popover="social.csdn" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">CSDN</span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">3</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">2</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">3</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Aurumting</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">访问人数</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">浏览总量</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>